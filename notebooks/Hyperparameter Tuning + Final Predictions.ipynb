{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40923621",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "# **1.** **Setup**\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cefd779",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5f416d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(221)\n",
    "random.seed(221)\n",
    "np.random.seed(221)\n",
    "tf.random.set_seed(221)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe1489a",
   "metadata": {},
   "source": [
    "## **1.1** Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f15595d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train/val split data\n",
    "with open('train_val_split.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Store the data in variables\n",
    "x_train = data['x_train']\n",
    "x_val = data['x_val']\n",
    "y_train = data['y_train']\n",
    "y_val = data['y_val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f7f5543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For EXTRA\n",
    "\n",
    "# Load the train/val split data without preprocessing\n",
    "with open('train_val_split_no_preproc.pkl', 'rb') as f:\n",
    "    data_no_preproc = pickle.load(f)\n",
    "\n",
    "# Convert DataFrames to list\n",
    "train_texts = data_no_preproc['x_train'].tolist()\n",
    "val_texts = data_no_preproc['x_val'].tolist()\n",
    "\n",
    "# Convert Series to list\n",
    "train_labels = data_no_preproc['y_train'].tolist()\n",
    "val_labels = data_no_preproc['y_val'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3262c5e3",
   "metadata": {},
   "source": [
    "## **1.2** Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab9ed8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- Parameters ------- \n",
    "# Glove\n",
    "emb_size = 50 # start small than increase to find the best value\n",
    "\n",
    "# TF-IDF\n",
    "max_df = 0.8 # we are removing terms that appear in >80% of tweets\n",
    "\n",
    "# Word2Vec\n",
    "window = 2         # context window size\n",
    "min_count = 1      # minimum word frequency to include\n",
    "\n",
    "# KNN\n",
    "n_neighbors = 10\n",
    "metric = 'cosine'\n",
    "weights = 'distance'\n",
    "\n",
    "# Naive Bayes\n",
    "alpha = 1\n",
    "\n",
    "# Random Forest\n",
    "n_estimators = 200\n",
    "criterion = 'gini'\n",
    "max_depth = 20 # to control overfitting\n",
    "min_samples_split = 10 # to avoid split on very small groups\n",
    "min_samples_leaf = 3 # to avoid very specific rules\n",
    "max_features = 'sqrt'\n",
    "class_weight = 'balanced'\n",
    "\n",
    "# Logistic Regression\n",
    "penalty = 'elasticnet'\n",
    "solver = 'saga'\n",
    "l1_ratio = 0.5\n",
    "C = 0.15\n",
    "class_weight = 'balanced'\n",
    "max_iter=300\n",
    "multi_class='multinomial'\n",
    "\n",
    "# XGBoost\n",
    "objective='multi:softmax'\n",
    "num_class=3\n",
    "eval_metric='mlogloss'\n",
    "use_label_encoder=False\n",
    "learning_rate = 0.05 # for better convergence\n",
    "max_depth_xgboost = 6\n",
    "n_estimators_xgboost = 300\n",
    "subsample = 0.8 # 80% of rows per tree to reduce overfitting\n",
    "colsample_bytree = 1\n",
    "scale_pos_weight = 1\n",
    "\n",
    "# LSTM\n",
    "batch_size=16\n",
    "epochs=10\n",
    "sg=1\n",
    "learning_rate_lstm=0.001\n",
    "optimizer=Adam(learning_rate=learning_rate_lstm)\n",
    "loss='categorical_crossentropy'\n",
    "metrics=['categorical_accuracy', Precision(name='precision'), Recall(name='recall'), AUC(name='auc', multi_label=True)]\n",
    "units=64\n",
    "dropout=0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5996a160",
   "metadata": {},
   "source": [
    "## **1.3** General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3198f952",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = x_train['text']\n",
    "\n",
    "#get list with lenghts of sentences\n",
    "train_len = []\n",
    "for i in corpus:\n",
    "    train_len.append(len(i))\n",
    "\n",
    "vector_size = max(train_len)\n",
    "\n",
    "metrics_df = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed1d3995",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = [word_tokenize(tweet.lower()) for tweet in x_train['text']]\n",
    "max_seq_len = max(len(tokens) for tokens in tokenized_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a2937c",
   "metadata": {},
   "source": [
    "## **1.4** Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dd7bc1",
   "metadata": {},
   "source": [
    "### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d2668b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'glove-twitter'\n",
    "glove_model = gensim.downloader.load(f'{model_name}-{emb_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b42d87b",
   "metadata": {},
   "source": [
    "### Text Embeddings 3 Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf05a972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AZURE_OPENAI_ENDPOINT: https://novaimsplayground.openai.azure.com/\n"
     ]
    }
   ],
   "source": [
    "# EXTRA\n",
    "\n",
    "# Load variables from .env into environment\n",
    "load_dotenv()\n",
    "\n",
    "# Print environment variable\n",
    "print(\"AZURE_OPENAI_ENDPOINT:\", os.getenv(\"AZURE_OPENAI_ENDPOINT\"))\n",
    "\n",
    "# Initialize Azure OpenAI client\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    api_version=\"2024-02-01\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "# Define embedding model\n",
    "model = \"text-embedding-3-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a00ad0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define embedding model text embedding\n",
    "model_te3s = \"text-embedding-3-small\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a05b68",
   "metadata": {},
   "source": [
    "### Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "665a7d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model and tokenizer for roberta\n",
    "model_name = \"cardiffnlp/twitter-roberta-base\"\n",
    "tokenizer_roberta = AutoTokenizer.from_pretrained(model_name)\n",
    "model_roberta = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461b8bdd",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db1231d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = Input(shape=(max_seq_len, vector_size))\n",
    "\n",
    "x = Masking(mask_value=0.0)(input_)\n",
    "x = Bidirectional(LSTM(units=units, return_sequences=False, dropout=dropout, recurrent_dropout=dropout))(x)\n",
    "x = Dropout(dropout)(x)\n",
    "output = Dense(num_class, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1020de10",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "# **2.** **Hyperparameter Tuning**\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0888fa",
   "metadata": {},
   "source": [
    "## **2.1** LR with word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e884f2",
   "metadata": {},
   "source": [
    "## **2.2** RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9066c247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer & Model\n",
    "checkpoint = \"cardiffnlp/twitter-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60dcb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversample\n",
    "train_texts_over, train_labels_over = oversample_data(train_texts, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5a66bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and convert to Dataset\n",
    "train_ds = Dataset.from_dict({\"text\": train_texts, \"label\": train_labels}).map(tokenize, batched=True)\n",
    "val_ds = Dataset.from_dict({\"text\": val_texts, \"label\": val_labels}).map(tokenize, batched=True)\n",
    "dataset = DatasetDict({\"train\": train_ds, \"validation\": val_ds})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dde1b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model init with weighted loss\n",
    "def model_init():\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=3)\n",
    "    class_weights = torch.tensor(\n",
    "        np.bincount(train_labels, minlength=3) / len(train_labels),\n",
    "        dtype=torch.float\n",
    "    )\n",
    "    class_weights = 1.0 / class_weights\n",
    "    model.classifier.loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44859808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search space\n",
    "def hp_space(trial: Trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True),\n",
    "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 4, 10),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.0, 0.3),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16, 32]),\n",
    "        \"warmup_ratio\": trial.suggest_float(\"warmup_ratio\", 0.0, 0.2)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e995fa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e912a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer for tuning\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    compute_metrics=compute_metrics_roberta\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeef5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "best_trial = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",\n",
    "    n_trials=15,\n",
    "    hp_space=hp_space,\n",
    "    backend=\"optuna\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe41403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain with best trial\n",
    "best_args = training_args.clone()\n",
    "for k, v in best_trial.hyperparameters.items():\n",
    "    setattr(best_args, k, v)\n",
    "\n",
    "best_model = model_init()\n",
    "best_trainer = Trainer(\n",
    "    model=best_model,\n",
    "    args=best_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    compute_metrics=compute_metrics_roberta\n",
    ")\n",
    "\n",
    "best_trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54b9e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for train and val sets\n",
    "train_preds = best_trainer.predict(dataset[\"train\"])\n",
    "val_preds = best_trainer.predict(dataset[\"validation\"])\n",
    "\n",
    "train_labels = train_preds.label_ids\n",
    "train_pred_labels = np.argmax(train_preds.predictions, axis=1)\n",
    "\n",
    "val_labels = val_preds.label_ids\n",
    "val_pred_labels = np.argmax(val_preds.predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9299f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Best RoBERTa Model Performance (Optuna)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575aa4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = get_metrics_df(title, train_labels, train_pred_labels, val_labels, val_pred_labels)\n",
    "display(metrics_df)\n",
    "\n",
    "plot_metrics(train_labels, train_pred_labels, val_labels, val_pred_labels, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc606e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"./best_roberta_model\"\n",
    "best_model.save_pretrained(output_path)\n",
    "tokenizer.save_pretrained(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03da179",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "# **3.** **Final Predictions**\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1887dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "test_data = pd.read_csv(\"../data/test.csv\")\n",
    "test_texts = test_data[\"text\"].tolist()\n",
    "\n",
    "# Get embeddings for test set\n",
    "X_test_roberta = np.array(get_roberta_embeddings(train_texts, \"X_test_roberta_embeddings.pkl\", batch_size=32, force_reload=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da07b55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels with the trained classifier\n",
    "X_test_te3s_pred = clf_roberta_lr.predict(X_test_te3s)\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission_te3s = pd.DataFrame({\n",
    "    \"id\": test_data[\"id\"],\n",
    "    \"label\": X_test_roberta_pred\n",
    "})\n",
    "\n",
    "# Save submission to CSV\n",
    "submission_te3s.to_csv(\"roberta_lr_pred_25.csv\", index=False)\n",
    "print(\"Submission file saved as roberta_lr_pred_25.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef85b5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the predictions from the saved CSV file\n",
    "pred_25 = pd.read_csv(\"roberta_lr_pred_25.csv\")\n",
    "pred_25.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
